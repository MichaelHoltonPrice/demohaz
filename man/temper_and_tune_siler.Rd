% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimization.R
\name{temper_and_tune_siler}
\alias{temper_and_tune_siler}
\title{Solve a minimization problem using parallel tempering and gradient descent
to fit the Siler hazard}
\usage{
temper_and_tune_siler(
  obj_fun,
  grad_fun,
  th0,
  verbose = FALSE,
  fn_plot = NULL,
  num_cyc = 100,
  samps_per_cyc = 20,
  temp_vect = 10^(rev(seq(-1, 1, by = 0.25))),
  prop_scale_mat = NULL,
  lr = 1e-05,
  func_tol = 1e-06,
  grad_tol = 0.01,
  miniter = 1,
  maxiter = 1000,
  report_period = 50,
  ...
)
}
\arguments{
\item{obj_fun}{The objective function to minimize}

\item{grad_fun}{The gradient function for the objective}

\item{th0}{The starting point for minimization}

\item{verbose}{Whether to print out optimization information
(default: FALSE)}

\item{fn_plot}{A function that can be used to plot progress. If NULL, no
plot is made}

\item{num_cyc}{Number of cycles for the tempering (default: 100)}

\item{samps_per_cyc}{Samples per cycle for the tempering (default: 20)}

\item{temp_vect}{Temperature vector for the tempering (default:
10^(rev(seq(-1,1,by=.25)))}

\item{prop_scale_mat}{Proposal scale matrix for the tempering (default:
ranges evenly from 0.1 for the hottest temperature to 0.001 for the
coldest temperature, for all parameter values at each temperature).}

\item{lr}{The learning rate to use
(default: 1e-5) [for gradient_descent]}

\item{func_tol}{A tolerance for changes in the objective function value. If
the change in value is less than the tolerance, optimization can halt,
though only if the grad_tol is condition is met (default: 1e-6).
[for gradient_descent]}

\item{grad_tol}{A tolerance for the absolute value of the gradient. All
gradients must be lower than this tolerance value. If rescale is TRUE,
the condition is applied to the rescaled gradient (default: 1e-2). Both
the func_tol and grad_tol conditions must be satisfied to halt.
[for gradient_descent]}

\item{miniter}{The minimum number of iterations to use (default: 1)
[for gradient_descent]}

\item{maxiter}{The maximum number of iterations to use (default: 1000)
[for gradient_descent]}

\item{report_period}{How often to update information, in steps. This is used
by both the tempering and gradient descent, but will not apply unless
verbose is TRUE or fn_plot is not NULL. (default: 50)}

\item{...}{Additional inputs to the objective function}
}
\value{
A list with the inputs (obj_fun, grad_fun, th0, and a list
  containing all the optional inputs), details on the tempering (temper),
  the best fit parameter vector from the tempering (th_temper), deatils on
  on the gradient descent (descent), and the final best-fit parameter
  vector from the gradient descent (th).
}
\description{
This method does robust function minimization by doing an initial
optimization using parallel tempering algorithm, then fine tuning using
vanilla gradient descent. The first input is the objective function to be
minimized (usually a negative log-likelihood) and the second input is a
function that calculates the gradient of the objective function. The third
input, an initial parameter vector for the temepring, is required. All
other inputs are optional. These optional inputs are control variables for
the two optimizations and switches and hooks to provide information on the
progress of the fit. There is a potential issue with statistical
identifiability stemming from the third, senescence term in the hazard.
Pragmatically, this can be identified by checking whether b[5] is large,
where 0.5 is a conservative cutoff to apply. If the best-fit parameter
vector from the tempering has b[5] > 0.5, a warning is thrown suggesting
that the user investigate further, and the fine tuning with gradient
descent is skipped.
}
