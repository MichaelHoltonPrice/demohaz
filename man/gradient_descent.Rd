% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimization.R
\name{gradient_descent}
\alias{gradient_descent}
\title{Vanilla gradient descent for demohaz}
\usage{
gradient_descent(
  th0,
  fn0,
  gr0,
  rescale = FALSE,
  lr = 1e-05,
  func_tol = 1e-06,
  grad_tol = 0.01,
  miniter = 1,
  maxiter = 10000,
  verbose = FALSE,
  fn_plot = NULL,
  report_period = 50,
  ...
)
}
\arguments{
\item{th0}{The starting point for minimization}

\item{fn0}{The objective function to minimize}

\item{gr0}{The gradient of the objective function to minimize}

\item{lr}{The learning rate to use for fine tuning using gradient descent
(default: 1e-5)}

\item{func_tol}{A tolerance for changes in the objective function value. If
the change in value is less than the tolerance, optimization can halt,
though only if the grad_tol is condition is met (default: 1e-6).}

\item{grad_tol}{A tolerance for the absolute value of the gradient. All
gradients must be lower than this tolerance value. If rescale is TRUE,
the condition is applied to the rescaled gradient (default: 1e-2). Both
the func_tol and grad_tol conditions must be satisfied to halt.}

\item{miniter}{The minimum number of iterations to use (default: 1)}

\item{maxiter}{The maximum number of iterations to use (default: 10000)}

\item{verbose}{Whether to print out optimization information
(default: FALSE)}

\item{fn_plot}{A function that can be used to plot over the histogram of
the data (xvalues/xcounts). If NULL, no plot is made
(default: FALSE)}

\item{report_period}{How often to update information, in steps. This is used
both for printing out information (if verbose is TRUE) and making a plot
(if fn_plot is not NULL) (default: 50)}

\item{...}{Additional inputs to fn0, gr0, and fn_plot}
}
\value{
A list containing par (the best-fit parameter value), value (the
  the corresponding objective function value), feval (the number of
  iterations/steps), and inputs (a list with the input variable values).
}
\description{
Do vanilla gradient descent. Optionally, if rescale is TRUE, the gradient
is calculated after dividing by the starting parameter vector par. There are
two reasons why demohaz implements its own optimziation algorithms. First,
it reduces dependenceies on third-party packages. Second, it allows
precise control over how to provide diagnostic information (e.g., printing
optimization information if verbose is TRUE and plotting the fit if
show_plot is TRUE).
}
